{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ffe005",
   "metadata": {},
   "source": [
    "# Silicon Wafer Fault Detection ML Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea02f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn(\"this will not show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baead70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"uci-secom.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1b8ad",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36631e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?', np.nan)\n",
    "df = df.apply(lambda x: x.fillna(0),axis=0)\n",
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e589a6",
   "metadata": {},
   "source": [
    "# Selecting unique features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.columns) - 2, -1, -1):\n",
    "    unique_vals = df.iloc[:,i].unique()\n",
    "    print(len(unique_vals))\n",
    "    if len(unique_vals)==1:\n",
    "        df.drop(df.columns[i],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f867f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the remaining features uniformly (excluding the last column)\n",
    "df.columns = [str(i) for i in range(1, df.shape[1])] + [df.columns[-1]]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73988f4",
   "metadata": {},
   "source": [
    "# Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afa789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cap_outliers_iqr(dataframe, features, cap_value=1.5):\n",
    "    for feature in features:\n",
    "        Q1 = dataframe[feature].quantile(0.25)\n",
    "        Q3 = dataframe[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - cap_value * IQR\n",
    "        upper_bound = Q3 + cap_value * IQR\n",
    "        dataframe[feature] = np.clip(dataframe[feature], lower_bound, upper_bound)\n",
    "    return dataframe\n",
    "\n",
    "all_features_to_process = [str(i) for i in range(1, 479)]  # Update the range\n",
    "\n",
    "df_capped = cap_outliers_iqr(df.copy(), all_features_to_process)\n",
    "\n",
    "plt.figure(figsize=(12, 15))\n",
    "\n",
    "# Box plot before capping outliers\n",
    "plt.subplot(2, 1, 1)\n",
    "df[all_features_to_process].boxplot()\n",
    "plt.title('Box Plot Before Capping Outliers (IQR)')\n",
    "\n",
    "# Box plot after capping outliers\n",
    "plt.subplot(2, 1, 2)\n",
    "df_capped[all_features_to_process].boxplot()\n",
    "plt.title('Box Plot After Capping Outliers (IQR)')\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# updating df\n",
    "df = df_capped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633198e",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Pass/Fail', axis=1)\n",
    "X = X.to_numpy()\n",
    "y = df[['Pass/Fail']]\n",
    "y = y.squeeze()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be422664",
   "metadata": {},
   "source": [
    "# Removing highly Correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e290d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_collinear_features(x, threshold):\n",
    "    # Convert NumPy array to Pandas DataFrame\n",
    "    x_df = pd.DataFrame(x, columns=[str(i) for i in range(1, x.shape[1] + 1)])\n",
    "\n",
    "    corr_matrix = x_df.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i + 1):\n",
    "            item = corr_matrix.iloc[j:(j + 1), (i + 1):(i + 2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "                drops = set(drop_cols)\n",
    "    x_df = x_df.drop(columns=drops)\n",
    "\n",
    "    # Convert DataFrame back to NumPy array\n",
    "    x_filtered = x_df.to_numpy()\n",
    "\n",
    "    return x_filtered\n",
    "\n",
    "# Assuming X is your NumPy array\n",
    "X_filtered = remove_collinear_features(X, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba022c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Shape after removing highly correlated features:\", X_filtered.shape)\n",
    "X=X_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73592842",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff313e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7c56a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "#print(explained_variance)\n",
    "cumulative_explained_variance = explained_variance.cumsum()\n",
    "#print(cumulative_explained_variance)\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74abb4",
   "metadata": {},
   "source": [
    "# Remove Dataset for Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you want to remove 10 rows from both classes\n",
    "num_rows_to_remove_per_class = 10\n",
    "\n",
    "# Check the shapes and counts before removal\n",
    "print(\"Original X size\", X.shape)\n",
    "print(\"Original y size\", y.shape)\n",
    "defected_count = y[y == -1].shape[0]\n",
    "not_defected_count = y[y == 1].shape[0]\n",
    "print(\"Defected count in original data:\", defected_count)\n",
    "print(\"Not defected count in original data:\", not_defected_count)\n",
    "\n",
    "# Identify the indices of rows to be removed for each class\n",
    "true_indices_remove = y[y == 1].sample(num_rows_to_remove_per_class, random_state=42).index\n",
    "false_indices_remove = y[y == -1].sample(num_rows_to_remove_per_class, random_state=42).index\n",
    "\n",
    "# Create a DataFrame to store the removed rows\n",
    "X_df=pd.DataFrame(X,index=y.index)\n",
    "removed_data = X_df.loc[pd.Index(true_indices_remove).union(false_indices_remove)].copy()\n",
    "removed_data=removed_data.join(y[true_indices_remove.union(false_indices_remove)])\n",
    "\n",
    "# Drop the selected rows from X and y\n",
    "X = X[~df.index.isin(true_indices_remove.union(false_indices_remove))]\n",
    "y = y[~df.index.isin(true_indices_remove.union(false_indices_remove))]\n",
    "\n",
    "# Check the new shapes and counts\n",
    "print(\"new X size\", X.shape)\n",
    "print(\"new y size\", y.shape)\n",
    "print(\"Removed data shape:\", removed_data.shape)\n",
    "\n",
    "defected_count = y[y == -1].shape[0]\n",
    "not_defected_count = y[y == 1].shape[0]\n",
    "print(\"Defected count in new data:\", defected_count)\n",
    "print(\"Not defected count in new data:\", not_defected_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f38b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(removed_data['Pass/Fail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_data['Pass/Fail'].replace(-1, 0, inplace=True)\n",
    "print(removed_data['Pass/Fail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8ec28",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "smt = SMOTETomek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d69e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "defected_count = y.to_list().count(0)\n",
    "not_defected_count =y.to_list().count(1)\n",
    "print(\"Data set::\")\n",
    "print(\"Defected count:\", defected_count)\n",
    "print(\"Not defected count:\", not_defected_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the dataset\n",
    "X, y = smt.fit_resample(X, y)\n",
    "# Print the balanced class distribution\n",
    "print(\"Class distribution after applying SMOTE to dataset:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "defected_count = y.to_list().count(0)\n",
    "not_defected_count =y.to_list().count(1)\n",
    "print(\"Data set::\")\n",
    "print(\"Defected count:\", defected_count)\n",
    "print(\"Not defected count:\", not_defected_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b4870",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cec85c",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcfe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Range of k values to test\n",
    "k_values = range(1, 50)  # You can adjust this range\n",
    "\n",
    "# Sum of squared distances for each k\n",
    "ssd = []\n",
    "\n",
    "# Fit K-Means for each k and compute sum of squared distances\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_train)\n",
    "    ssd.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, ssd, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the K-Means clustering algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a K-Means model with a specified number of clusters\n",
    "n_clusters = 5 # You can adjust the number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "# Fit the K-Means model to your data\n",
    "kmeans.fit(X_train)  # Use the training data for clustering\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "train_cluster_labels = kmeans.predict(X_train)\n",
    "\n",
    "# Plot the clusters in 2D\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_train[:, 0], y=X_train[:, 1], hue=train_cluster_labels, palette='viridis')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd6cb7",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00194f9f",
   "metadata": {},
   "source": [
    "# Find Optimal value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "error_rate = []\n",
    "for i in range(1,50):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "    \n",
    "plt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb727590",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = min(error_rate)  # Find the smallest value in the list\n",
    "min_k = error_rate.index(min_value) \n",
    "print(min_value)\n",
    "print(min_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "knn = KNeighborsClassifier(n_neighbors=min_k)\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "train_accuracy_k = knn.score(X_train, y_train)\n",
    "test_accuracy_k = knn.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_k)\n",
    "print(\"Test Accuracy: \",test_accuracy_k)\n",
    "\n",
    "accuracy_k = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_k=classification_report(y_test,pred)\n",
    "print(report_k)\n",
    "lines = report_k.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee6895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "f1_score_line = lines[2]\n",
    "f1_score_k= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_k)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "precision_k= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_k)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "recall_k= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_k)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr,thresholds=roc_curve(y_test,pred)\n",
    "plt.plot(fpr,tpr,label='ROC_curve(AUC={:.2f})'.format(auc_roc))\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a1593",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "  \n",
    "classifier = LogisticRegression(max_iter=500)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(X_test)\n",
    "\n",
    "train_accuracy_l1 = classifier.score(X_train, y_train)\n",
    "test_accuracy_l1 = classifier.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_l1)\n",
    "print(\"Test Accuracy: \",test_accuracy_l1)\n",
    "\n",
    "accuracy_l1 = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f174f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_l1=classification_report(y_test,pred)\n",
    "print(report_l1)\n",
    "lines = report_l1.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d92ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2]\n",
    "f1_score_l1= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_l1)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "precision_l1= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_l1)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "recall_l1= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_l1)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a983220",
   "metadata": {},
   "source": [
    "# Boosting For Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier = LogisticRegression(max_iter=500)\n",
    "boosted_classifier = AdaBoostClassifier(base_estimator=classifier, n_estimators=200)\n",
    "boosted_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred = boosted_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879559b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_accuracy_l = boosted_classifier.score(X_train, y_train)\n",
    "test_accuracy_l = boosted_classifier.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_l)\n",
    "print(\"Test Accuracy: \",test_accuracy_l)\n",
    "\n",
    "accuracy_l = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_l=classification_report(y_test,pred)\n",
    "print(report_l)\n",
    "lines = report_l.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2]\n",
    "f1_score_l= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_l)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "precision_l= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_l)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "recall_l= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_l)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd454fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds=roc_curve(y_test,pred)\n",
    "plt.plot(fpr,tpr,label='ROC_curve(AUC={:.2f})'.format(auc_roc))\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5fffe",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ee4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "naive_bayes_classifier = BernoulliNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "pred = naive_bayes_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c97c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_n1 = naive_bayes_classifier.score(X_train, y_train)\n",
    "test_accuracy_n1 = naive_bayes_classifier.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_n1)\n",
    "print(\"Test Accuracy: \",test_accuracy_n1)\n",
    "\n",
    "accuracy_n1 = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_n1=classification_report(y_test,pred)\n",
    "print(report_n1)\n",
    "lines = report_n1.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16379858",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2]\n",
    "f1_score_n1= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_n1)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "precision_n1= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_n1)\n",
    "\n",
    "f1_score_line = lines[2]  \n",
    "recall_n1= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_n1)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726626dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc_roc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdbf42",
   "metadata": {},
   "source": [
    "# Boosting for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "naive_bayes = BernoulliNB()\n",
    "boosted_naive_bayes = AdaBoostClassifier(base_estimator=naive_bayes, n_estimators=200)\n",
    "\n",
    "boosted_naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "pred = boosted_naive_bayes.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78918be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_n = boosted_naive_bayes.score(X_train, y_train)\n",
    "test_accuracy_n = boosted_naive_bayes.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_n)\n",
    "print(\"Test Accuracy: \",test_accuracy_n)\n",
    "\n",
    "accuracy_n = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_n=classification_report(y_test,pred)\n",
    "print(report_n)\n",
    "lines = report_n.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2]\n",
    "f1_score_n= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_n)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "precision_n= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_n)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "recall_n= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_n)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0686ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc_roc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0f1f3",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2965d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(max_depth=10) \n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "train_accuracy_d1 = dtree.score(X_train, y_train)\n",
    "test_accuracy_d1 = dtree.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_d1)\n",
    "print(\"Test Accuracy: \",test_accuracy_d1)\n",
    "\n",
    "accuracy_d1 = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(100,80))  # Adjust the figure size as needed\n",
    "plot_tree(dtree, feature_names=df.columns[:-1], class_names=[\"Not Defected\", \"Defected\"], filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82385b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_d1=classification_report(y_test,pred)\n",
    "print(report_d1)\n",
    "lines = report_d1.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a942776",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2]\n",
    "f1_score_d1= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_d1)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "precision_d1= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_d1)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "recall_d1= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_d1)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69063e2d",
   "metadata": {},
   "source": [
    "# Gradient Boosting Ensembling for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c89a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create the Stochastic Gradient Boosting classifier\n",
    "sgb = GradientBoostingClassifier(n_estimators=200)\n",
    "\n",
    "# Train the classifier\n",
    "sgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "pred = sgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34637cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_d = sgb.score(X_train, y_train)\n",
    "test_accuracy_d = sgb.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_d)\n",
    "print(\"Test Accuracy: \",test_accuracy_d)\n",
    "\n",
    "accuracy_d = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8751cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_d=classification_report(y_test,pred)\n",
    "print(report_d)\n",
    "lines = report_d.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2]\n",
    "f1_score_d= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_d)\n",
    "\n",
    "f1_score_line = lines[2]\n",
    "precision_d= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_d)\n",
    "\n",
    "f1_score_line = lines[2] \n",
    "recall_d= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_d)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc_roc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0c10e",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_r = rf_classifier.score(X_train, y_train)\n",
    "test_accuracy_r = rf_classifier.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_r)\n",
    "print(\"Test Accuracy: \",test_accuracy_r)\n",
    "\n",
    "accuracy_r = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b10ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_r=classification_report(y_test,pred)\n",
    "print(report_r)\n",
    "lines = report_r.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0913024",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2] \n",
    "f1_score_r= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_r)\n",
    "\n",
    "f1_score_line = lines[2]  \n",
    "precision_r= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_r)\n",
    "\n",
    "f1_score_line = lines[2]  \n",
    "recall_r= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_r)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f17c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc_roc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba58c5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='rbf')\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred = svm_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be203875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_s = svm_classifier.score(X_train, y_train)\n",
    "test_accuracy_s = svm_classifier.score(X_test, y_test)\n",
    "print(\"Train Accuracy: \",train_accuracy_s)\n",
    "print(\"Test Accuracy: \",test_accuracy_s)\n",
    "\n",
    "accuracy_s = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy: \",accuracy_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a992b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test,pred))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_s=classification_report(y_test,pred)\n",
    "print(report_s)\n",
    "lines = report_s.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8757d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_line = lines[2] \n",
    "f1_score_s= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_s)\n",
    "\n",
    "f1_score_line = lines[2]  \n",
    "precision_s= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_s)\n",
    "\n",
    "f1_score_line = lines[2]  \n",
    "recall_s= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_s)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc_roc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_mlp_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    initializer = HeUniform() \n",
    "    model.add(Dense(512, input_shape=(100,), kernel_initializer=initializer))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(128, kernel_initializer=initializer))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    " \n",
    "\n",
    "    model.add(Dense(128, kernel_initializer=initializer))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    " \n",
    "    model.add(Dense(128, kernel_initializer=initializer))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    " \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = custom_mlp_model()\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=20, verbose=1, validation_data=(X_test, y_test))\n",
    "test_loss_nn, test_accuracy_nn = model.evaluate(X_test, y_test)\n",
    "train_accuracy_nn = max(history.history['accuracy'])\n",
    "\n",
    "print(\"Train Accuracy of neural network: \", train_accuracy_nn)\n",
    "print(\"Test Accuracy of neural network: \", test_accuracy_nn)\n",
    "\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "rounded_predictions = tf.where(predictions >= 0.5, 1.0, 0.0)\n",
    "y_pred=rounded_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rounded_predictions = np.round(predictions, 2)\n",
    "print(np.unique(rounded_predictions))\n",
    "\n",
    "print(np.unique(y_pred))\n",
    "print(np.unique(y_test))\n",
    "train_accuracy = history.history['accuracy']\n",
    "test_accuracy = history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Print the final accuracies\n",
    "final_train_accuracy = train_accuracy[-1]\n",
    "final_test_accuracy = test_accuracy[-1]\n",
    "\n",
    "print(f\"Final Train Accuracy: {final_train_accuracy*100:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {final_test_accuracy*100:.2f}%\")\n",
    "accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy\", accuracy_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "report_nn = classification_report(y_test,y_pred)\n",
    "print(report_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report_nn.split('\\n')\n",
    "f1_score_line = lines[2] \n",
    "f1_score_nn= float(f1_score_line.split()[3])\n",
    "print(\"F1 Score:\", f1_score_nn)\n",
    "\n",
    "f1_score_line = lines[2]  \n",
    "precision_nn= float(f1_score_line.split()[1])\n",
    "print(\"Precision:\", precision_nn)\n",
    "\n",
    "f1_score_line = lines[2]  \n",
    "recall_nn= float(f1_score_line.split()[2])\n",
    "print(\"Recall:\", recall_nn)\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc_roc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guess line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947b598",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['KNN', 'Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random Forest','SVM','Neural Network']\n",
    "\n",
    "values = [train_accuracy_k , train_accuracy_l, train_accuracy_n, train_accuracy_d, train_accuracy_r,train_accuracy_s,train_accuracy_nn]\n",
    "\n",
    "# Data for the columns\n",
    "labels = ['KNN', 'Logistic Regression', 'Naive Bayes','Decision Tree', 'Random Forest', 'SVM','Neural Network']\n",
    "values1 = [train_accuracy_k , train_accuracy_l, train_accuracy_n, train_accuracy_d, train_accuracy_r, train_accuracy_s,train_accuracy_nn]\n",
    "values2 = [test_accuracy_k , test_accuracy_l, test_accuracy_n, test_accuracy_d, test_accuracy_r, test_accuracy_s,test_accuracy_nn]\n",
    "values3 = [accuracy_k , accuracy_l, accuracy_n, accuracy_d, accuracy_r, accuracy_s,accuracy_nn]\n",
    "\n",
    "# Positions of the bars on the x-axis\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "# Width of each bar\n",
    "width = 0.15\n",
    "\n",
    "# Plotting the bar graph\n",
    "#print(values1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(x - width, values1, width, label='Train Accuracy')\n",
    "plt.bar(x, values2, width, label='Test Accuracy')\n",
    "plt.bar(x + width, values3, width, label='Accuracy')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Bar Graph with accuracies')\n",
    "\n",
    "# Setting the x-axis tick labels\n",
    "plt.xticks(x, labels)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Displaying the graph\n",
    "plt.show()\n",
    "classes = ['KNN', 'Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'SVM','Neural Network']\n",
    "values = [f1_score_k,f1_score_l ,f1_score_n ,f1_score_d, f1_score_r, f1_score_s,f1_score_nn]\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plotting the bar graph\n",
    "plt.bar(classes, values)\n",
    "\n",
    "# Setting the y-axis limits to range from 0 to 1\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Adding labels and title\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('f1_score')\n",
    "plt.title('Bar Graph of f1-Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "# Define the columns\n",
    "table.field_names = [\"Models\", \"Precision\" , \"Recall\" , \"F1-Score\" , \"Train Accuracy\", \"Test Accuracy\", \"Accuracy\"]\n",
    "# Add rows to the table\n",
    "table.add_row([\"KNN\", precision_k , recall_k , f1_score_k , train_accuracy_k , test_accuracy_k , accuracy_k ])\n",
    "table.add_row([\"Logistic Regression\", precision_l1 , recall_l1 , f1_score_l1 , train_accuracy_l1 , test_accuracy_l1 , accuracy_l1 ])\n",
    "table.add_row([\"LR with boosting\", precision_l , recall_l , f1_score_l , train_accuracy_l , test_accuracy_l , accuracy_l ])\n",
    "table.add_row([\"Naive Bayes\", precision_n1 , recall_n1 , f1_score_n1 , train_accuracy_n1 , test_accuracy_n1 , accuracy_n1 ])\n",
    "table.add_row([\"NB with boosting\", precision_n , recall_n , f1_score_n , train_accuracy_n , test_accuracy_n , accuracy_n ])\n",
    "table.add_row([\"Decision Tree\", precision_d1 , recall_d1 , f1_score_d1 , train_accuracy_d1 , test_accuracy_d1 , accuracy_d1 ])\n",
    "table.add_row([\"DT with boosting\", precision_d , recall_d , f1_score_d , train_accuracy_d , test_accuracy_d , accuracy_d ])\n",
    "table.add_row([\"Random Forest\", precision_r , recall_r , f1_score_r , train_accuracy_r , test_accuracy_r , accuracy_r ])\n",
    "table.add_row([\"SVM\", precision_s , recall_s , f1_score_s , train_accuracy_s , test_accuracy_s , accuracy_s ])\n",
    "table.add_row([\"Neural Network\", precision_nn , recall_nn , f1_score_nn , train_accuracy_nn , test_accuracy_nn , accuracy_nn ])\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a14e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"#################################################Classification Report###########################################\")\n",
    "# Create a PrettyTable instance\n",
    "table1 = PrettyTable()\n",
    "\n",
    "# Define columns and set column widths\n",
    "table1.field_names = [\"kNN\", \"Logistic Regression\"]\n",
    "table1._max_width = {\"Column 1\": 90, \"Column 2\": 90}  # Adjust width as needed\n",
    "table1.add_row([report_k, report_l1])\n",
    "print(table1)\n",
    "table2 = PrettyTable()\n",
    "table2.field_names = ['Logistic Regression with boosting','Naive Bayes']\n",
    "table2._max_width = {\"Column 1\": 90, \"Column 2\": 90}  # Adjust width as needed\n",
    "table2.add_row([report_l, report_n1])\n",
    "print(table2)\n",
    "table3 = PrettyTable()\n",
    "table3.field_names = ['Naive Bayes with boosting','Decision Tree']\n",
    "table3._max_width = {\"Column 1\": 90, \"Column 2\": 90}  # Adjust width as needed\n",
    "table3.add_row([report_n, report_d1])\n",
    "print(table3)\n",
    "table4 = PrettyTable()\n",
    "table4.field_names = ['Decision Tree with boosting', 'Random Forest']\n",
    "table4._max_width = {\"Column 1\": 90, \"Column 2\": 90}  # Adjust width as needed\n",
    "table4.add_row([report_d, report_r])\n",
    "print(table4)\n",
    "table5 = PrettyTable()\n",
    "table5.field_names = ['SVM', 'Neural Network']\n",
    "table5._max_width = {\"Column 1\": 90, \"Column 2\": 90}  # Adjust width as needed\n",
    "table5.add_row([report_s,report_nn])\n",
    "print(table5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5faf2",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for inferencing\n",
    "X_i=removed_data.drop('Pass/Fail', axis=1)\n",
    "y_i=removed_data['Pass/Fail']\n",
    "y_i = y_i.values\n",
    "\n",
    "# Predictions from trained models\n",
    "inference_k = knn.predict(X_i)\n",
    "inference_l = boosted_classifier.predict(X_i)\n",
    "inference_n = boosted_naive_bayes.predict(X_i)\n",
    "inference_d = sgb.predict(X_i)\n",
    "inference_r = rf_classifier.predict(X_i)\n",
    "inference_s = svm_classifier.predict(X_i)\n",
    "inference_nn = model.predict(X_i)\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "inference_nn_binary = [1 if prediction > threshold else 0 for prediction in inference_nn]\n",
    "\n",
    "# Convert the list to a NumPy array and print without commas and spaces\n",
    "inference_nn_binary = np.array(inference_nn_binary)\n",
    "\n",
    "#comparing True value and predicted value\n",
    "true_predictions_nn = (inference_nn_binary == y_i)\n",
    "true_predictions_k = (inference_k == y_i)\n",
    "true_predictions_l = (inference_l == y_i)\n",
    "true_predictions_n = (inference_n == y_i)\n",
    "true_predictions_d = (inference_d == y_i)\n",
    "true_predictions_r = (inference_r == y_i)\n",
    "true_predictions_s = (inference_s == y_i)\n",
    "\n",
    "# Calculate and display the number of true and false predictions\n",
    "num_total_predictions=23\n",
    "\n",
    "num_true_predictions_k = sum(true_predictions_k)\n",
    "num_false_predictions_k = len(inference_k) - num_true_predictions_k\n",
    "per_k = (num_true_predictions_k / num_total_predictions) * 100\n",
    "\n",
    "num_true_predictions_l = sum(true_predictions_l)\n",
    "num_false_predictions_l = len(inference_l) - num_true_predictions_l\n",
    "per_l = (num_true_predictions_l / num_total_predictions) * 100\n",
    "\n",
    "num_true_predictions_n = sum(true_predictions_n)\n",
    "num_false_predictions_n = len(inference_n) - num_true_predictions_n\n",
    "per_n = (num_true_predictions_n / num_total_predictions) * 100\n",
    "\n",
    "num_true_predictions_d = sum(true_predictions_d)\n",
    "num_false_predictions_d = len(inference_d) - num_true_predictions_d\n",
    "per_d = (num_true_predictions_d / num_total_predictions) * 100\n",
    "\n",
    "num_true_predictions_r = sum(true_predictions_r)\n",
    "num_false_predictions_r = len(inference_r) - num_true_predictions_r\n",
    "per_r=(num_true_predictions_r / num_total_predictions) * 100\n",
    "\n",
    "num_true_predictions_s = sum(true_predictions_s)\n",
    "num_false_predictions_s = len(inference_s) - num_true_predictions_s\n",
    "per_s=(num_true_predictions_s / num_total_predictions) * 100\n",
    "\n",
    "num_true_predictions_nn = sum(true_predictions_nn)\n",
    "num_false_predictions_nn = len(inference_nn) - num_true_predictions_nn\n",
    "per_nn = (num_true_predictions_nn / num_total_predictions) * 100\n",
    "\n",
    "# Display output\n",
    "print(\"                    Inferencing for 23 Data points\")\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "data = {'Num of True pred': [num_true_predictions_k, num_true_predictions_l, num_true_predictions_n, num_true_predictions_d, num_true_predictions_r, num_true_predictions_s, num_true_predictions_nn],\n",
    "        'Num of False pred':[num_false_predictions_k, num_false_predictions_l, num_false_predictions_n, num_false_predictions_d, num_false_predictions_r,  num_false_predictions_s, num_false_predictions_nn],\n",
    "        '% Correct': [per_k, per_l, per_n, per_d, per_r, per_s,per_nn]}\n",
    "row_indices = ['KNN', 'Logistic Regression', 'Naive Bayes','Decision Tree', 'Random Forest' , 'SVM' , 'Neural Network']\n",
    "df_final= pd.DataFrame(data, index=row_indices)\n",
    "\n",
    "print(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"                                             KNN\")\n",
    "print(\"Predicted: \",inference_k)\n",
    "print(\"Actual   : \",y_i)\n",
    "print(\"                                     Logistic Regression\")\n",
    "print(\"Predicted: \",inference_l)\n",
    "print(\"Actual   : \",y_i)\n",
    "print(\"                                         Naive Bayes\")\n",
    "print(\"Predicted: \",inference_n)\n",
    "print(\"Actual   : \",y_i)\n",
    "print(\"                                        Decision Tree\")\n",
    "print(\"Predicted: \",inference_d)\n",
    "print(\"Actual   : \",y_i)\n",
    "print(\"                                        Random Forest\")\n",
    "print(\"Predicted: \",inference_r)\n",
    "print(\"Actual   : \",y_i)\n",
    "print(\"                                             SVM\")\n",
    "print(\"Predicted: \",inference_s)\n",
    "print(\"Actual   : \",y_i)\n",
    "print(\"                                     Neural Network\")\n",
    "print(\"Predicted: \",inference_nn_binary)\n",
    "print(\"Actual   : \",y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d565b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot ROC curve for a model\n",
    "def plot_roc_curve(model_name, y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_value:.2f})')\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "plot_roc_curve(\"KNN\", y_test, knn.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"Logistic Regression\", y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"Boosted Logistic Regression\", y_test, boosted_classifier.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"Naive Bayes\", y_test, naive_bayes_classifier.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"Boosted Naive Bayes\", y_test, boosted_naive_bayes.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"Decision Tree\", y_test, dtree.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"Boosted Decision Tree\", y_test, boosted_classifier.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"Random Forest\", y_test, rf_classifier.predict_proba(X_test)[:, 1])\n",
    "plot_roc_curve(\"SVM\", y_test, svm_classifier.decision_function(X_test))\n",
    "plot_roc_curve(\"Neural Network\", y_test, model.predict(X_test).ravel())\n",
    "\n",
    "# Customize the plot\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
